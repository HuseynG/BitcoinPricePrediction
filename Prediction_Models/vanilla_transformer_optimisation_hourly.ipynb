{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformer import VanillaTimeSeriesTransformer_EncoderOnly\n",
    "from utils import Trainer, preprocess_data, seed_everything\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss, L1Loss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Transformer (Next Step Prediction | Encoder only)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamer Tuning (Structural Tuning Too) for 2 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>forex_sentiment_score</th>\n",
       "      <th>forex_total</th>\n",
       "      <th>stock_sentiment_score</th>\n",
       "      <th>stock_total</th>\n",
       "      <th>btc_sentiment_score</th>\n",
       "      <th>btc_Total</th>\n",
       "      <th>...</th>\n",
       "      <th>NG=F</th>\n",
       "      <th>SI=F</th>\n",
       "      <th>ZW=F</th>\n",
       "      <th>DFF</th>\n",
       "      <th>CPIAUCSL</th>\n",
       "      <th>SMA_24_hourly</th>\n",
       "      <th>RSI_24_hourly</th>\n",
       "      <th>MACD_hourly</th>\n",
       "      <th>SMA_168_hourly</th>\n",
       "      <th>RSI_168_hourly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7151</td>\n",
       "      <td>28.512000</td>\n",
       "      <td>648.925</td>\n",
       "      <td>0.04</td>\n",
       "      <td>227.842</td>\n",
       "      <td>4.580000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.105703</td>\n",
       "      <td>5.570774</td>\n",
       "      <td>71.315357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7151</td>\n",
       "      <td>28.512000</td>\n",
       "      <td>648.925</td>\n",
       "      <td>0.04</td>\n",
       "      <td>227.842</td>\n",
       "      <td>4.580000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.105703</td>\n",
       "      <td>5.570774</td>\n",
       "      <td>71.315357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7151</td>\n",
       "      <td>28.512000</td>\n",
       "      <td>648.925</td>\n",
       "      <td>0.04</td>\n",
       "      <td>227.842</td>\n",
       "      <td>4.580000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.105703</td>\n",
       "      <td>5.570774</td>\n",
       "      <td>71.315357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7151</td>\n",
       "      <td>28.512000</td>\n",
       "      <td>648.925</td>\n",
       "      <td>0.04</td>\n",
       "      <td>227.842</td>\n",
       "      <td>4.580000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.105703</td>\n",
       "      <td>5.570774</td>\n",
       "      <td>71.315357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7151</td>\n",
       "      <td>28.512000</td>\n",
       "      <td>648.925</td>\n",
       "      <td>0.04</td>\n",
       "      <td>227.842</td>\n",
       "      <td>4.580000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.105703</td>\n",
       "      <td>5.570774</td>\n",
       "      <td>71.315357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99788</th>\n",
       "      <td>27025.00</td>\n",
       "      <td>27097.00</td>\n",
       "      <td>27016.00</td>\n",
       "      <td>27082.00</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>139.0</td>\n",
       "      <td>0.278</td>\n",
       "      <td>2365.0</td>\n",
       "      <td>0.407</td>\n",
       "      <td>118.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3180</td>\n",
       "      <td>25.009001</td>\n",
       "      <td>603.750</td>\n",
       "      <td>4.83</td>\n",
       "      <td>303.294</td>\n",
       "      <td>26954.071429</td>\n",
       "      <td>57.095624</td>\n",
       "      <td>40.279134</td>\n",
       "      <td>27033.416667</td>\n",
       "      <td>48.544716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99789</th>\n",
       "      <td>27045.00</td>\n",
       "      <td>27060.00</td>\n",
       "      <td>26983.00</td>\n",
       "      <td>27023.00</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>139.0</td>\n",
       "      <td>0.278</td>\n",
       "      <td>2365.0</td>\n",
       "      <td>0.407</td>\n",
       "      <td>118.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3180</td>\n",
       "      <td>25.009001</td>\n",
       "      <td>603.750</td>\n",
       "      <td>4.83</td>\n",
       "      <td>303.294</td>\n",
       "      <td>26967.071429</td>\n",
       "      <td>58.517086</td>\n",
       "      <td>42.195293</td>\n",
       "      <td>27034.452381</td>\n",
       "      <td>48.636546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99790</th>\n",
       "      <td>27077.00</td>\n",
       "      <td>27082.00</td>\n",
       "      <td>27022.00</td>\n",
       "      <td>27041.00</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>139.0</td>\n",
       "      <td>0.278</td>\n",
       "      <td>2365.0</td>\n",
       "      <td>0.407</td>\n",
       "      <td>118.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3180</td>\n",
       "      <td>25.009001</td>\n",
       "      <td>603.750</td>\n",
       "      <td>4.83</td>\n",
       "      <td>303.294</td>\n",
       "      <td>26978.428571</td>\n",
       "      <td>60.757339</td>\n",
       "      <td>45.768407</td>\n",
       "      <td>27035.464286</td>\n",
       "      <td>48.783669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99791</th>\n",
       "      <td>27115.00</td>\n",
       "      <td>27139.00</td>\n",
       "      <td>27061.00</td>\n",
       "      <td>27073.00</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>139.0</td>\n",
       "      <td>0.278</td>\n",
       "      <td>2365.0</td>\n",
       "      <td>0.407</td>\n",
       "      <td>118.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3180</td>\n",
       "      <td>25.009001</td>\n",
       "      <td>603.750</td>\n",
       "      <td>4.83</td>\n",
       "      <td>303.294</td>\n",
       "      <td>26994.500000</td>\n",
       "      <td>63.292474</td>\n",
       "      <td>51.077616</td>\n",
       "      <td>27037.404762</td>\n",
       "      <td>48.958322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99792</th>\n",
       "      <td>27123.00</td>\n",
       "      <td>27123.00</td>\n",
       "      <td>27123.00</td>\n",
       "      <td>27123.00</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>139.0</td>\n",
       "      <td>0.278</td>\n",
       "      <td>2365.0</td>\n",
       "      <td>0.407</td>\n",
       "      <td>118.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3180</td>\n",
       "      <td>25.009001</td>\n",
       "      <td>603.750</td>\n",
       "      <td>4.83</td>\n",
       "      <td>303.294</td>\n",
       "      <td>27010.500000</td>\n",
       "      <td>63.822350</td>\n",
       "      <td>55.293349</td>\n",
       "      <td>27039.553571</td>\n",
       "      <td>48.995158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99793 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          close      high       low      open  forex_sentiment_score  \\\n",
       "0          4.58      4.58      4.58      4.58                   0.00   \n",
       "1          4.58      4.58      4.58      4.58                   0.00   \n",
       "2          4.58      4.58      4.58      4.58                   0.00   \n",
       "3          4.58      4.58      4.58      4.58                   0.00   \n",
       "4          4.58      4.58      4.58      4.58                   0.00   \n",
       "...         ...       ...       ...       ...                    ...   \n",
       "99788  27025.00  27097.00  27016.00  27082.00                  -0.27   \n",
       "99789  27045.00  27060.00  26983.00  27023.00                  -0.27   \n",
       "99790  27077.00  27082.00  27022.00  27041.00                  -0.27   \n",
       "99791  27115.00  27139.00  27061.00  27073.00                  -0.27   \n",
       "99792  27123.00  27123.00  27123.00  27123.00                  -0.27   \n",
       "\n",
       "       forex_total  stock_sentiment_score  stock_total  btc_sentiment_score  \\\n",
       "0              0.0                  0.000          0.0                0.000   \n",
       "1              0.0                  0.000          0.0                0.000   \n",
       "2              0.0                  0.000          0.0                0.000   \n",
       "3              0.0                  0.000          0.0                0.000   \n",
       "4              0.0                  0.000          0.0                0.000   \n",
       "...            ...                    ...          ...                  ...   \n",
       "99788        139.0                  0.278       2365.0                0.407   \n",
       "99789        139.0                  0.278       2365.0                0.407   \n",
       "99790        139.0                  0.278       2365.0                0.407   \n",
       "99791        139.0                  0.278       2365.0                0.407   \n",
       "99792        139.0                  0.278       2365.0                0.407   \n",
       "\n",
       "       btc_Total  ...    NG=F       SI=F     ZW=F   DFF  CPIAUCSL  \\\n",
       "0            0.0  ...  2.7151  28.512000  648.925  0.04   227.842   \n",
       "1            0.0  ...  2.7151  28.512000  648.925  0.04   227.842   \n",
       "2            0.0  ...  2.7151  28.512000  648.925  0.04   227.842   \n",
       "3            0.0  ...  2.7151  28.512000  648.925  0.04   227.842   \n",
       "4            0.0  ...  2.7151  28.512000  648.925  0.04   227.842   \n",
       "...          ...  ...     ...        ...      ...   ...       ...   \n",
       "99788      118.0  ...  2.3180  25.009001  603.750  4.83   303.294   \n",
       "99789      118.0  ...  2.3180  25.009001  603.750  4.83   303.294   \n",
       "99790      118.0  ...  2.3180  25.009001  603.750  4.83   303.294   \n",
       "99791      118.0  ...  2.3180  25.009001  603.750  4.83   303.294   \n",
       "99792      118.0  ...  2.3180  25.009001  603.750  4.83   303.294   \n",
       "\n",
       "       SMA_24_hourly  RSI_24_hourly  MACD_hourly  SMA_168_hourly  \\\n",
       "0           4.580000     100.000000     0.105703        5.570774   \n",
       "1           4.580000     100.000000     0.105703        5.570774   \n",
       "2           4.580000     100.000000     0.105703        5.570774   \n",
       "3           4.580000     100.000000     0.105703        5.570774   \n",
       "4           4.580000     100.000000     0.105703        5.570774   \n",
       "...              ...            ...          ...             ...   \n",
       "99788   26954.071429      57.095624    40.279134    27033.416667   \n",
       "99789   26967.071429      58.517086    42.195293    27034.452381   \n",
       "99790   26978.428571      60.757339    45.768407    27035.464286   \n",
       "99791   26994.500000      63.292474    51.077616    27037.404762   \n",
       "99792   27010.500000      63.822350    55.293349    27039.553571   \n",
       "\n",
       "       RSI_168_hourly  \n",
       "0           71.315357  \n",
       "1           71.315357  \n",
       "2           71.315357  \n",
       "3           71.315357  \n",
       "4           71.315357  \n",
       "...               ...  \n",
       "99788       48.544716  \n",
       "99789       48.636546  \n",
       "99790       48.783669  \n",
       "99791       48.958322  \n",
       "99792       48.995158  \n",
       "\n",
       "[99793 rows x 48 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([89781, 36, 2])\n",
      "torch.Size([89781, 1, 1])\n",
      "                                                       \n",
      "    device: cuda,\n",
      "    num_heads:  8,\n",
      "    d_model: 336,\n",
      "    num_layers: 4,\n",
      "    dff: 1030,\n",
      "    mlp_size: 47,\n",
      "    dropout_rate: 0.27140072343193467,\n",
      "    mlp_dropout_rate: 0.1761659400132718\n",
      "Current MAE 0.2634594228474159                         \n",
      "Best MAE inf                                           \n",
      "Model saved. MAE: 0.2634594228474159                   \n",
      "Current MAE 0.2434671028384994                         \n",
      "Best MAE 0.2634594228474159                            \n",
      "Model saved. MAE: 0.2434671028384994                   \n",
      "Current MAE 0.25996684491538774                        \n",
      "Best MAE 0.2434671028384994                            \n",
      "  0%|          | 0/100 [03:09<?, ?trial/s, best loss=?]"
     ]
    }
   ],
   "source": [
    "df = df[[\"close\", \"open\"]]\n",
    "input_seq_len_ = 36\n",
    "output_seq_len_ = 1\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_)\n",
    "def objective(params):\n",
    "    print(f\"\"\"\n",
    "    device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')},\n",
    "    num_heads:  {params['num_heads']},\n",
    "    d_model: {params['d_model_by_num_heads'] * params['num_heads']},\n",
    "    num_layers: {params['num_layers']},\n",
    "    dff: {params['dff']},\n",
    "    mlp_size: {params['mlp_size']},\n",
    "    dropout_rate: {params['dropout_rate']},\n",
    "    mlp_dropout_rate: {params['mlp_dropout_rate']}\"\"\")\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Use the parameters to define your model, train it and evaluate it.\n",
    "    num_heads = params['num_heads']\n",
    "    d_model = params['d_model_by_num_heads'] * num_heads\n",
    "    model = VanillaTimeSeriesTransformer_EncoderOnly(\n",
    "        num_features=int(len(scaled_df.columns)),\n",
    "        num_layers=params['num_layers'],\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dff=params['dff'],\n",
    "        input_seq_len=36,\n",
    "        output_seq_len=1,\n",
    "        mlp_size=params['mlp_size'],\n",
    "        dropout_rate=params['dropout_rate'],\n",
    "        mlp_dropout_rate=params['mlp_dropout_rate']\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimiser = Adam(model.parameters(), lr=params['lr'])\n",
    "    scheduler = ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "    criterion = MSELoss()\n",
    "                 \n",
    "    model_trainer = Trainer(model=model,\n",
    "                    train_dataloader=train_dataloader,\n",
    "                    val_dataloader=val_dataloader,\n",
    "                    test_dataloader=test_dataloader,\n",
    "                    criterion=criterion,\n",
    "                    optimiser=optimiser,\n",
    "                    scheduler=scheduler,\n",
    "                    device=device,\n",
    "                    num_epochs=50,\n",
    "                    early_stopping_patience_limit=10,\n",
    "                    is_save_model=True,\n",
    "                    scaler=scaler_close)\n",
    "\n",
    "    train_losses, val_losses = model_trainer.train_loop()\n",
    "\n",
    "    # Return the last validation loss from the training loop\n",
    "    return {'loss': val_losses[-1], 'status': STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'num_layers': scope.int(hp.quniform('num_layers', 1, 8, 1)),\n",
    "    'num_heads': scope.int(hp.quniform('num_heads', 1, 8, 1)),\n",
    "    'd_model_by_num_heads': scope.int(hp.quniform('d_model_by_num_heads', 32, 64, 2)),\n",
    "    'dff': scope.int(hp.quniform('dff', 2, 2048, 1)),\n",
    "    'mlp_size': scope.int(hp.quniform('mlp_size', 32, 64, 1)),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.1, 0.3),\n",
    "    'mlp_dropout_rate': hp.uniform('mlp_dropout_rate', 0.1, 0.3),\n",
    "    'lr': hp.loguniform('lr', np.log(0.0001), np.log(0.1))\n",
    "}\n",
    "\n",
    "# Run the optimization\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(best)\n",
    "\n",
    "with open('../data/stats_on_hyperparam_for_two_cols_vanilla_transformer_hourly_encoder_only.pkl', 'wb') as file:\n",
    "    pickle.dump(best, file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "with open('../data/stats_on_hyperparam_for_two_cols_vanilla_transformer_hourly_encoder_only.pkl', 'rb') as file:\n",
    "    best = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len_ = 36\n",
    "output_seq_len_ = 1\n",
    "stats = {}\n",
    "for cols in df.columns:\n",
    "  if cols != \"close\":\n",
    "    print(cols)\n",
    "    # prepare temp_df\n",
    "    temp_df = df[[\"close\", cols]]\n",
    "\n",
    "    # preprocesing temp_df\n",
    "    scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(temp_df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_)\n",
    "\n",
    "    # instantiate model (after hyperparameter tuning)\n",
    "    \n",
    "    num_features = int(len(scaled_df.columns)) # a.k.a, number of cols in df\n",
    "    num_layers = int(best[\"num_layers\"])\n",
    "    num_heads = int(best[\"num_heads\"])\n",
    "    d_model = int(best['d_model_by_num_heads']) * num_heads\n",
    "    dff = int(best['dff'])\n",
    "    mlp_size = int(best['mlp_size']) # size of the first MLP layer\n",
    "    dropout_rate = round(best['dropout_rate'], 3)  # dropout rate for the Transformer layers\n",
    "    mlp_dropout_rate = round(best['mlp_dropout_rate'], 3) # dropout rate for the MLP layers\n",
    "\n",
    "    # instantiating model\n",
    "    model = VanillaTimeSeriesTransformer_EncoderOnly(num_features, num_layers, d_model, num_heads, dff, input_seq_len_,\n",
    "                                  output_seq_len_, mlp_size, dropout_rate, mlp_dropout_rate)\n",
    "\n",
    "    # moving the model to the device (GPU if available)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    criterion = MSELoss()\n",
    "    optimiser = Adam(model.parameters(), lr=round(best['lr'], 6))\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "\n",
    "    # declaring trainer object\n",
    "    model_trainer = Trainer(model=model,\n",
    "                  train_dataloader=train_dataloader,\n",
    "                  val_dataloader=val_dataloader,\n",
    "                  test_dataloader=test_dataloader,\n",
    "                  criterion=criterion,\n",
    "                  optimiser=optimiser,\n",
    "                  scheduler=scheduler,\n",
    "                  device=device,\n",
    "                  num_epochs=50,\n",
    "                  early_stopping_patience_limit=10,\n",
    "                  is_save_model=True,\n",
    "                  scaler=scaler_close)\n",
    "    # training\n",
    "    train_losses, val_losses = model_trainer.train_loop()\n",
    "    # testing\n",
    "    mse, mae = model_trainer.test_model()\n",
    "\n",
    "    stats[cols] = {\n",
    "        \"mse\":mse,\n",
    "        \"mae\":mae\n",
    "    }\n",
    "\n",
    "with open('../data/stats_on_features_vanilla_transformer_hourly.pkl', 'wb') as file:\n",
    "    pickle.dump(stats, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/stats_on_features_vanilla_transformer_hourly.pkl', 'rb') as file:\n",
    "    loaded_stats = pickle.load(file)\n",
    "sorted_loaded_stats = OrderedDict(sorted(loaded_stats.items(), key=lambda item: item[1]['mse']))\n",
    "\n",
    "# pprint(sorted_loaded_stats)\n",
    "\n",
    "count = 0\n",
    "top_feattures = []\n",
    "for k,v in sorted_loaded_stats.items():\n",
    "  if count < 10:\n",
    "    top_feattures.append(k)\n",
    "    count += 1\n",
    "print(top_feattures)\n",
    "with open('../data/top_feattures.pkl', 'wb') as file:\n",
    "    pickle.dump(top_feattures, file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamer Tuning (Structural Tuning Too) for Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "with open('../data/top_feattures.pkl', 'rb') as file:\n",
    "    top_feattures = pickle.load(file)\n",
    "top_feattures.append(\"close\")\n",
    "df = df[top_feattures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len_ = 36\n",
    "output_seq_len_ = 1\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_)\n",
    "def objective(params):\n",
    "    print(f\"\"\"\n",
    "    device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')},\n",
    "    num_heads:  {params['num_heads']},\n",
    "    d_model: {params['d_model_by_num_heads'] * params['num_heads']},\n",
    "    num_layers: {params['num_layers']},\n",
    "    dff: {params['dff']},\n",
    "    mlp_size: {params['mlp_size']},\n",
    "    dropout_rate: {params['dropout_rate']},\n",
    "    mlp_dropout_rate: {params['mlp_dropout_rate']}\"\"\")\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Use the parameters to define your model, train it and evaluate it.\n",
    "    num_heads = params['num_heads']\n",
    "    d_model = params['d_model_by_num_heads'] * num_heads\n",
    "    model = VanillaTimeSeriesTransformer_EncoderOnly(\n",
    "        num_features=int(len(scaled_df.columns)),\n",
    "        num_layers=params['num_layers'],\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dff=params['dff'],\n",
    "        input_seq_len=36,\n",
    "        output_seq_len=1,\n",
    "        mlp_size=params['mlp_size'],\n",
    "        dropout_rate=params['dropout_rate'],\n",
    "        mlp_dropout_rate=params['mlp_dropout_rate']\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimiser = Adam(model.parameters(), lr=params['lr'])\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=5)\n",
    "    criterion = MSELoss()\n",
    "\n",
    "    model_trainer = Trainer(model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=val_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                criterion=criterion,\n",
    "                optimiser=optimiser,\n",
    "                scheduler=scheduler,\n",
    "                device=device,\n",
    "                num_epochs=50,\n",
    "                early_stopping_patience_limit=10,\n",
    "                is_save_model=True,\n",
    "                scaler=scaler_close)\n",
    "\n",
    "    train_losses, val_losses = model_trainer.train_loop()\n",
    "\n",
    "    # Return the last validation loss from the training loop\n",
    "    return {'loss': val_losses[-1], 'status': STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'num_layers': scope.int(hp.quniform('num_layers', 1, 8, 1)),\n",
    "    'num_heads': scope.int(hp.quniform('num_heads', 1, 8, 1)),\n",
    "    'd_model_by_num_heads': scope.int(hp.quniform('d_model_by_num_heads', 32, 64, 2)),\n",
    "    'dff': scope.int(hp.quniform('dff', 2, 2048, 1)),\n",
    "    'mlp_size': scope.int(hp.quniform('mlp_size', 32, 64, 1)),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.1, 0.3),\n",
    "    'mlp_dropout_rate': hp.uniform('mlp_dropout_rate', 0.1, 0.3),\n",
    "    'lr': hp.loguniform('lr', np.log(0.0001), np.log(0.1))\n",
    "}\n",
    "\n",
    "# Run the optimization\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(best)\n",
    "\n",
    "with open('../data/best_on_hyperparam_for_top_Feature_Combo_vanilla_transformer_hourly.pkl', 'wb') as file:\n",
    "    pickle.dump(best, file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation of Vanilla Transformer (Encoder only) on Close Price, Top 2, 5, 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close Price Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "with open('../data/top_feattures.pkl', 'rb') as file:\n",
    "    top_feattures = pickle.load(file)\n",
    "# top_feattures = top_feattures[:1]\n",
    "top_feattures.append(\"close\")\n",
    "df = df[[\"close\"]]\n",
    "# df\n",
    "input_seq_len_ = 36\n",
    "output_seq_len_ = 1\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_)\n",
    "\n",
    "\n",
    "with open('../data/best_on_hyperparam_for_top_Feature_Combo_vanilla_transformer_hourly.pkl', 'rb') as file:\n",
    "    best = pickle.load(file)\n",
    "\n",
    "# instantiating model (after hyperparameter tuning)\n",
    "\n",
    "num_features = int(len(scaled_df.columns)) # a.k.a, number of cols in df\n",
    "num_layers = int(best[\"num_layers\"])\n",
    "num_heads = int(best[\"num_heads\"])\n",
    "d_model = int(best['d_model_by_num_heads']) * num_heads\n",
    "dff = int(best['dff'])\n",
    "mlp_size = int(best['mlp_size']) # size of the first MLP layer\n",
    "dropout_rate = round(best['dropout_rate'], 3)  # dropout rate for the Transformer layers\n",
    "mlp_dropout_rate = round(best['mlp_dropout_rate'], 3) # dropout rate for the MLP layers\n",
    "\n",
    "# instantiating model\n",
    "model = VanillaTimeSeriesTransformer_EncoderOnly(num_features, num_layers, d_model, num_heads, dff, input_seq_len_,\n",
    "                                output_seq_len_, mlp_size, dropout_rate, mlp_dropout_rate)\n",
    "\n",
    "# moving the model to the device (GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "criterion = MSELoss() # L1Loss()\n",
    "optimiser = Adam(model.parameters(), lr=round(best['lr'], 6))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "\n",
    "# declaring trainer object\n",
    "model_trainer = Trainer(model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            criterion=criterion,\n",
    "            optimiser=optimiser,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            num_epochs=20,\n",
    "            early_stopping_patience_limit=10,\n",
    "            is_save_model=True,\n",
    "            scaler=scaler_close)\n",
    "# training\n",
    "train_losses, val_losses = model_trainer.train_loop(is_plot=True, is_plot_and_plot_test=True)\n",
    "# testing\n",
    "mse, mae = model_trainer.test_model()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "with open('../data/top_feattures.pkl', 'rb') as file:\n",
    "    top_feattures = pickle.load(file)\n",
    "top_feattures = top_feattures[:2]\n",
    "top_feattures.append(\"close\")\n",
    "df = df[top_feattures]\n",
    "input_seq_len_ = 36\n",
    "output_seq_len_ = 1\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_)\n",
    "\n",
    "\n",
    "with open('../data/best_on_hyperparam_for_top_Feature_Combo_vanilla_transformer_hourly.pkl', 'rb') as file:\n",
    "    best = pickle.load(file)\n",
    "\n",
    "# instantiating model (after hyperparameter tuning)\n",
    "\n",
    "num_features = int(len(scaled_df.columns)) # a.k.a, number of cols in df\n",
    "num_layers = int(best[\"num_layers\"])\n",
    "num_heads = int(best[\"num_heads\"])\n",
    "d_model = int(best['d_model_by_num_heads']) * num_heads\n",
    "dff = int(best['dff'])\n",
    "mlp_size = int(best['mlp_size']) # size of the first MLP layer\n",
    "dropout_rate = round(best['dropout_rate'], 3)  # dropout rate for the Transformer layers\n",
    "mlp_dropout_rate = round(best['mlp_dropout_rate'], 3) # dropout rate for the MLP layers\n",
    "\n",
    "# instantiating model\n",
    "model = VanillaTimeSeriesTransformer_EncoderOnly(num_features, num_layers, d_model, num_heads, dff, input_seq_len_,\n",
    "                                output_seq_len_, mlp_size, dropout_rate, mlp_dropout_rate)\n",
    "\n",
    "# moving the model to the device (GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "criterion = MSELoss() # L1Loss()\n",
    "optimiser = Adam(model.parameters(), lr=round(best['lr'], 6))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "\n",
    "# declaring trainer object\n",
    "model_trainer = Trainer(model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            criterion=criterion,\n",
    "            optimiser=optimiser,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            num_epochs=20,\n",
    "            early_stopping_patience_limit=10,\n",
    "            is_save_model=True,\n",
    "            scaler=scaler_close)\n",
    "# training\n",
    "train_losses, val_losses = model_trainer.train_loop(is_plot=True, is_plot_and_plot_test=True)\n",
    "# testing\n",
    "mse, mae = model_trainer.test_model()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 5 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "with open('../data/top_feattures.pkl', 'rb') as file:\n",
    "    top_feattures = pickle.load(file)\n",
    "top_feattures = top_feattures[:5]\n",
    "top_feattures.append(\"close\")\n",
    "df = df[top_feattures]\n",
    "input_seq_len_ = 36\n",
    "output_seq_len_ = 1\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_)\n",
    "\n",
    "\n",
    "with open('../data/best_on_hyperparam_for_top_Feature_Combo_vanilla_transformer_hourly.pkl', 'rb') as file:\n",
    "    best = pickle.load(file)\n",
    "\n",
    "# instantiating model (after hyperparameter tuning)\n",
    "\n",
    "num_features = int(len(scaled_df.columns)) # a.k.a, number of cols in df\n",
    "num_layers = int(best[\"num_layers\"])\n",
    "num_heads = int(best[\"num_heads\"])\n",
    "d_model = int(best['d_model_by_num_heads']) * num_heads\n",
    "dff = int(best['dff'])\n",
    "mlp_size = int(best['mlp_size']) # size of the first MLP layer\n",
    "dropout_rate = round(best['dropout_rate'], 3)  # dropout rate for the Transformer layers\n",
    "mlp_dropout_rate = round(best['mlp_dropout_rate'], 3) # dropout rate for the MLP layers\n",
    "\n",
    "# instantiating model\n",
    "model = VanillaTimeSeriesTransformer_EncoderOnly(num_features, num_layers, d_model, num_heads, dff, input_seq_len_,\n",
    "                                output_seq_len_, mlp_size, dropout_rate, mlp_dropout_rate)\n",
    "\n",
    "# moving the model to the device (GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "criterion = MSELoss() # L1Loss()\n",
    "optimiser = Adam(model.parameters(), lr=round(best['lr'], 6))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "\n",
    "# declaring trainer object\n",
    "model_trainer = Trainer(model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=val_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                criterion=criterion,\n",
    "                optimiser=optimiser,\n",
    "                scheduler=scheduler,\n",
    "                device=device,\n",
    "                num_epochs=20,\n",
    "                early_stopping_patience_limit=10,\n",
    "                is_save_model=True,\n",
    "                scaler=scaler_close)\n",
    "# training\n",
    "train_losses, val_losses = model_trainer.train_loop(is_plot=True, is_plot_and_plot_test=True)\n",
    "# testing\n",
    "mse, mae = model_trainer.test_model()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "with open('../data/top_feattures.pkl', 'rb') as file:\n",
    "    top_feattures = pickle.load(file)\n",
    "top_feattures.append(\"close\")\n",
    "df = df[top_feattures]\n",
    "input_seq_len_ = 36\n",
    "output_seq_len_ = 1\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_)\n",
    "\n",
    "\n",
    "with open('../data/best_on_hyperparam_for_top_Feature_Combo_vanilla_transformer_hourly.pkl', 'rb') as file:\n",
    "    best = pickle.load(file)\n",
    "\n",
    "# instantiating model (after hyperparameter tuning)\n",
    "\n",
    "num_features = int(len(scaled_df.columns)) # a.k.a, number of cols in df\n",
    "num_layers = int(best[\"num_layers\"])\n",
    "num_heads = int(best[\"num_heads\"])\n",
    "d_model = int(best['d_model_by_num_heads']) * num_heads\n",
    "dff = int(best['dff'])\n",
    "mlp_size = int(best['mlp_size']) # size of the first MLP layer\n",
    "dropout_rate = round(best['dropout_rate'], 3)  # dropout rate for the Transformer layers\n",
    "mlp_dropout_rate = round(best['mlp_dropout_rate'], 3) # dropout rate for the MLP layers\n",
    "\n",
    "# instantiating model\n",
    "model = VanillaTimeSeriesTransformer_EncoderOnly(num_features, num_layers, d_model, num_heads, dff, input_seq_len_,\n",
    "                                output_seq_len_, mlp_size, dropout_rate, mlp_dropout_rate)\n",
    "\n",
    "# moving the model to the device (GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "criterion = MSELoss() # L1Loss()\n",
    "optimiser = Adam(model.parameters(), lr=round(best['lr'], 6))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "\n",
    "# declaring trainer object\n",
    "model_trainer = Trainer(model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            criterion=criterion,\n",
    "            optimiser=optimiser,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            num_epochs=20,\n",
    "            early_stopping_patience_limit=10,\n",
    "            is_save_model=True,\n",
    "            scaler=scaler_close)\n",
    "# training\n",
    "train_losses, val_losses = model_trainer.train_loop(is_plot=True, is_plot_and_plot_test=True)\n",
    "# testing\n",
    "mse, mae = model_trainer.test_model()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Transformer (Next 24th Step Prediction | Encoder only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "# training\n",
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "with open('../data/top_feattures.pkl', 'rb') as file:\n",
    "    top_feattures = pickle.load(file)\n",
    "# top_feattures = top_feattures[:5]\n",
    "# top_feattures.append(\"close\")\n",
    "df = df[[\"close\"]]\n",
    "# df = df[top_feattures]\n",
    "input_seq_len_ = 168\n",
    "output_seq_len_ = 24\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_,\n",
    "                                                                                                             output_as_seq=False)\n",
    "\n",
    "\n",
    "with open('../data/best_on_hyperparam_for_top_Feature_Combo_vanilla_transformer_hourly.pkl', 'rb') as file:\n",
    "    best = pickle.load(file)\n",
    "\n",
    "# instantiating model (after hyperparameter tuning)\n",
    "\n",
    "num_features = int(len(scaled_df.columns)) # a.k.a, number of cols in df\n",
    "num_layers = int(best[\"num_layers\"])\n",
    "num_heads = int(best[\"num_heads\"])\n",
    "d_model = int(best['d_model_by_num_heads']) * num_heads\n",
    "dff = int(best['dff'])\n",
    "mlp_size = int(best['mlp_size']) # size of the first MLP layer\n",
    "dropout_rate = round(best['dropout_rate'], 3)  # dropout rate for the Transformer layers\n",
    "mlp_dropout_rate = round(best['mlp_dropout_rate'], 3) # dropout rate for the MLP layers\n",
    "\n",
    "# instantiating model\n",
    "model = VanillaTimeSeriesTransformer_EncoderOnly(num_features, num_layers, d_model, num_heads, dff, input_seq_len_,\n",
    "                                output_seq_len_, mlp_size, dropout_rate, mlp_dropout_rate)\n",
    "\n",
    "# moving the model to the device (GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "criterion = MSELoss() # L1Loss()\n",
    "optimiser = Adam(model.parameters(), lr=round(best['lr'], 6))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "\n",
    "# declaring trainer object\n",
    "model_trainer = Trainer(model=model,\n",
    "                  train_dataloader=train_dataloader,\n",
    "                  val_dataloader=val_dataloader,\n",
    "                  test_dataloader=test_dataloader,\n",
    "                  criterion=criterion,\n",
    "                  optimiser=optimiser,\n",
    "                  scheduler=scheduler,\n",
    "                  device=device,\n",
    "                  num_epochs=20,\n",
    "                  early_stopping_patience_limit=10,\n",
    "                  is_save_model=True,\n",
    "                  scaler=scaler_close)\n",
    "\n",
    "# training\n",
    "train_losses, val_losses = model_trainer.train_loop(is_plot=True, is_plot_and_plot_test=True)\n",
    "# testing\n",
    "mse, mae = model_trainer.test_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete this afterwards\n",
    "# data preprocessing\n",
    "# training\n",
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "with open('../data/top_feattures.pkl', 'rb') as file:\n",
    "    top_feattures = pickle.load(file)\n",
    "# top_feattures = top_feattures[:5]\n",
    "# top_feattures.append(\"close\")\n",
    "df = df[[\"close\"]]\n",
    "# df = df[top_feattures]\n",
    "input_seq_len_ = 168\n",
    "output_seq_len_ = 1\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_,\n",
    "                                                                                                             output_as_seq=False)\n",
    "\n",
    "\n",
    "with open('../data/best_on_hyperparam_for_top_Feature_Combo_vanilla_transformer_hourly.pkl', 'rb') as file:\n",
    "    best = pickle.load(file)\n",
    "\n",
    "# instantiating model (after hyperparameter tuning)\n",
    "\n",
    "num_features = int(len(scaled_df.columns)) # a.k.a, number of cols in df\n",
    "num_layers = int(best[\"num_layers\"])\n",
    "num_heads = int(best[\"num_heads\"])\n",
    "d_model = int(best['d_model_by_num_heads']) * num_heads\n",
    "dff = int(best['dff'])\n",
    "mlp_size = int(best['mlp_size']) # size of the first MLP layer\n",
    "dropout_rate = round(best['dropout_rate'], 3)  # dropout rate for the Transformer layers\n",
    "mlp_dropout_rate = round(best['mlp_dropout_rate'], 3) # dropout rate for the MLP layers\n",
    "\n",
    "# instantiating model\n",
    "model = VanillaTimeSeriesTransformer_EncoderOnly(num_features, num_layers, d_model, num_heads, dff, input_seq_len_,\n",
    "                                output_seq_len_, mlp_size, dropout_rate, mlp_dropout_rate)\n",
    "\n",
    "# moving the model to the device (GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = MSELoss() # L1Loss()\n",
    "optimiser = Adam(model.parameters(), lr=round(best['lr'], 6))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "\n",
    "# declaring trainer object\n",
    "model_trainer = Trainer(model=model,\n",
    "                  train_dataloader=train_dataloader,\n",
    "                  val_dataloader=val_dataloader,\n",
    "                  test_dataloader=test_dataloader,\n",
    "                  criterion=criterion,\n",
    "                  optimiser=optimiser,\n",
    "                  scheduler=scheduler,\n",
    "                  device=device,\n",
    "                  num_epochs=50,\n",
    "                  early_stopping_patience_limit=5,\n",
    "                  is_save_model=True,\n",
    "                  scaler=scaler_close)\n",
    "\n",
    "# training\n",
    "train_losses, val_losses = model_trainer.train_loop(is_plot=True, is_plot_and_plot_test=True)\n",
    "# testing\n",
    "mse, mae = model_trainer.test_model()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bitcoinproject1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
