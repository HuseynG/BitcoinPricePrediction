{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformer import VanillaTimeSeriesTransformer_EncoderOnly\n",
    "from utils import Trainer, preprocess_data, seed_everything\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss, L1Loss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Transformer (Next Step Prediction | Encoder only)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning (Structural Tuning Too) for 2 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"close\", \"open\"]]\n",
    "input_seq_len_ = 36\n",
    "output_seq_len_ = 1\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_)\n",
    "def objective(params):\n",
    "    start_time = int(time.time())\n",
    "    print(f\"\"\"\n",
    "    device: {torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')},\n",
    "    num_heads:  {params['num_heads']},\n",
    "    d_model: {params['d_model_by_num_heads'] * params['num_heads']},\n",
    "    num_layers: {params['num_layers']},\n",
    "    dff: {params['dff']},\n",
    "    mlp_size: {params['mlp_size']},\n",
    "    dropout_rate: {params['dropout_rate']},\n",
    "    mlp_dropout_rate: {params['mlp_dropout_rate']}\"\"\")\n",
    "\n",
    "    device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "    # Use the parameters to define your model, train it and evaluate it.\n",
    "    num_heads = params['num_heads']\n",
    "    d_model = params['d_model_by_num_heads'] * num_heads\n",
    "    model = VanillaTimeSeriesTransformer_EncoderOnly(\n",
    "        num_features=int(len(scaled_df.columns)),\n",
    "        num_layers=params['num_layers'],\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dff=params['dff'],\n",
    "        mlp_size=params['mlp_size'],\n",
    "        dropout_rate=params['dropout_rate'],\n",
    "        mlp_dropout_rate=params['mlp_dropout_rate']\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimiser = Adam(model.parameters(), lr=params['lr'])\n",
    "    scheduler = ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "    criterion = MSELoss()\n",
    "                 \n",
    "    model_trainer = Trainer(model=model,\n",
    "                    train_dataloader=train_dataloader,\n",
    "                    val_dataloader=val_dataloader,\n",
    "                    test_dataloader=test_dataloader,\n",
    "                    criterion=criterion,\n",
    "                    optimiser=optimiser,\n",
    "                    scheduler=scheduler,\n",
    "                    device=device,\n",
    "                    num_epochs=50,\n",
    "                    early_stopping_patience_limit=10,\n",
    "                    is_save_model=True,\n",
    "                    scaler=scaler_close,\n",
    "                    file_path = f\"models/best_model_{start_time}.pt\")\n",
    "\n",
    "    train_losses, val_losses = model_trainer.train_loop()\n",
    "\n",
    "    # Return the last validation loss from the training loop\n",
    "    return {'loss': val_losses[-1], 'status': STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'num_layers': scope.int(hp.quniform('num_layers', 1, 8, 1)),\n",
    "    'num_heads': scope.int(hp.quniform('num_heads', 1, 8, 1)),\n",
    "    'd_model_by_num_heads': scope.int(hp.quniform('d_model_by_num_heads', 32, 64, 2)),\n",
    "    'dff': scope.int(hp.quniform('dff', 2, 2048, 50)),\n",
    "    'mlp_size': scope.int(hp.quniform('mlp_size', 32, 64, 2)),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.1, 0.3),\n",
    "    'mlp_dropout_rate': hp.uniform('mlp_dropout_rate', 0.1, 0.3),\n",
    "    'lr': hp.loguniform('lr', np.log(0.0001), np.log(0.1))\n",
    "}\n",
    "\n",
    "# Run the optimization\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(best)\n",
    "\n",
    "with open('../data/stats_on_hyperparam_for_two_cols_vanilla_transformer_hourly_encoder_only.pkl', 'wb') as file:\n",
    "    pickle.dump(best, file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "with open('../data/stats_on_hyperparam_for_two_cols_vanilla_transformer_hourly_encoder_only.pkl', 'rb') as file:\n",
    "    best = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len_ = 36\n",
    "output_seq_len_ = 1\n",
    "stats = {}\n",
    "for cols in df.columns:\n",
    "  if cols != \"close\":\n",
    "    print(cols)\n",
    "    # prepare temp_df\n",
    "    temp_df = df[[\"close\", cols]]\n",
    "\n",
    "    # preprocesing temp_df\n",
    "    scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(temp_df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_)\n",
    "\n",
    "    # instantiate model (after hyperparameter tuning)\n",
    "    num_features = int(len(scaled_df.columns)) # a.k.a, number of cols in df\n",
    "    num_layers = int(best[\"num_layers\"])\n",
    "    num_heads = int(best[\"num_heads\"])\n",
    "    d_model = int(best['d_model_by_num_heads']) * num_heads\n",
    "    dff = int(best['dff'])\n",
    "    mlp_size = int(best['mlp_size']) # size of the first MLP layer\n",
    "    dropout_rate = round(best['dropout_rate'], 3)  # dropout rate for the Transformer layers\n",
    "    mlp_dropout_rate = round(best['mlp_dropout_rate'], 3) # dropout rate for the MLP layers\n",
    "\n",
    "    # instantiating model\n",
    "    model = VanillaTimeSeriesTransformer_EncoderOnly(num_features, num_layers, d_model, num_heads, dff,\n",
    "                                                      mlp_size, dropout_rate, mlp_dropout_rate)\n",
    "\n",
    "    # moving the model to the device (GPU if available)\n",
    "    device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    criterion = MSELoss()\n",
    "    optimiser = Adam(model.parameters(), lr=round(best['lr'], 6))\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "\n",
    "    # declaring trainer object\n",
    "    model_trainer = Trainer(model=model,\n",
    "                    train_dataloader=train_dataloader,\n",
    "                    val_dataloader=val_dataloader,\n",
    "                    test_dataloader=test_dataloader,\n",
    "                    criterion=criterion,\n",
    "                    optimiser=optimiser,\n",
    "                    scheduler=scheduler,\n",
    "                    device=device,\n",
    "                    num_epochs=50,\n",
    "                    early_stopping_patience_limit=10,\n",
    "                    is_save_model=True,\n",
    "                    scaler=scaler_close,\n",
    "                    file_path = \"models/best_model.pt\")\n",
    "    # training\n",
    "    train_losses, val_losses = model_trainer.train_loop()\n",
    "    # testing\n",
    "    mse, mae = model_trainer.test_model()\n",
    "\n",
    "    stats[cols] = {\n",
    "        \"mse\":mse,\n",
    "        \"mae\":mae\n",
    "    }\n",
    "\n",
    "with open('../data/stats_on_features_vanilla_transformer_hourly.pkl', 'wb') as file:\n",
    "    pickle.dump(stats, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/stats_on_features_vanilla_transformer_hourly.pkl', 'rb') as file:\n",
    "    loaded_stats = pickle.load(file)\n",
    "sorted_loaded_stats = OrderedDict(sorted(loaded_stats.items(), key=lambda item: item[1]['mse']))\n",
    "\n",
    "# pprint(sorted_loaded_stats)\n",
    "\n",
    "count = 0\n",
    "top_feattures = []\n",
    "for k,v in sorted_loaded_stats.items():\n",
    "  if count < 10:\n",
    "    top_feattures.append(k)\n",
    "    count += 1\n",
    "print(top_feattures)\n",
    "with open('../data/top_feattures.pkl', 'wb') as file:\n",
    "    pickle.dump(top_feattures, file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning (Structural Tuning Too) for Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "with open('../data/top_feattures.pkl', 'rb') as file:\n",
    "    top_feattures = pickle.load(file)\n",
    "top_feattures.append(\"close\")\n",
    "df = df[top_feattures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len_ = 36\n",
    "output_seq_len_ = 1\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_)\n",
    "def objective(params):\n",
    "    start_time = int(time.time())\n",
    "    print(f\"\"\"\n",
    "    device: {torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')},\n",
    "    num_heads:  {params['num_heads']},\n",
    "    d_model: {params['d_model_by_num_heads'] * params['num_heads']},\n",
    "    num_layers: {params['num_layers']},\n",
    "    dff: {params['dff']},\n",
    "    mlp_size: {params['mlp_size']},\n",
    "    dropout_rate: {params['dropout_rate']},\n",
    "    mlp_dropout_rate: {params['mlp_dropout_rate']}\"\"\")\n",
    "\n",
    "    device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "    # Use the parameters to define your model, train it and evaluate it.\n",
    "    num_heads = params['num_heads']\n",
    "    d_model = params['d_model_by_num_heads'] * num_heads\n",
    "    model = VanillaTimeSeriesTransformer_EncoderOnly(\n",
    "        num_features=int(len(scaled_df.columns)),\n",
    "        num_layers=params['num_layers'],\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dff=params['dff'],\n",
    "        mlp_size=params['mlp_size'],\n",
    "        dropout_rate=params['dropout_rate'],\n",
    "        mlp_dropout_rate=params['mlp_dropout_rate']\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimiser = Adam(model.parameters(), lr=params['lr'])\n",
    "    scheduler = ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "    criterion = MSELoss()\n",
    "\n",
    "    model_trainer = Trainer(model=model,\n",
    "                    train_dataloader=train_dataloader,\n",
    "                    val_dataloader=val_dataloader,\n",
    "                    test_dataloader=test_dataloader,\n",
    "                    criterion=criterion,\n",
    "                    optimiser=optimiser,\n",
    "                    scheduler=scheduler,\n",
    "                    device=device,\n",
    "                    num_epochs=50,\n",
    "                    early_stopping_patience_limit=10,\n",
    "                    is_save_model=True,\n",
    "                    scaler=scaler_close,\n",
    "                    file_path = f\"models/best_model_{start_time}.pt\")\n",
    "\n",
    "    train_losses, val_losses = model_trainer.train_loop()\n",
    "\n",
    "    # Return the last validation loss from the training loop\n",
    "    return {'loss': val_losses[-1], 'status': STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'num_layers': scope.int(hp.quniform('num_layers', 1, 8, 1)),\n",
    "    'num_heads': scope.int(hp.quniform('num_heads', 1, 8, 1)),\n",
    "    'd_model_by_num_heads': scope.int(hp.quniform('d_model_by_num_heads', 32, 64, 2)),\n",
    "    'dff': scope.int(hp.quniform('dff', 2, 2048, 50)),\n",
    "    'mlp_size': scope.int(hp.quniform('mlp_size', 32, 64, 2)),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.1, 0.3),\n",
    "    'mlp_dropout_rate': hp.uniform('mlp_dropout_rate', 0.1, 0.3),\n",
    "    'lr': hp.loguniform('lr', np.log(0.0001), np.log(0.1))\n",
    "}\n",
    "\n",
    "# Run the optimization\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(best)\n",
    "\n",
    "with open('../data/best_on_hyperparam_for_top_Feature_Combo_vanilla_transformer_hourly.pkl', 'wb') as file:\n",
    "    pickle.dump(best, file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation of Vanilla Transformer (Encoder only) on Close Price, Top 2, 5, 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close Price Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "with open('../data/top_feattures.pkl', 'rb') as file:\n",
    "    top_feattures = pickle.load(file)\n",
    "# top_feattures = top_feattures[:1]\n",
    "top_feattures.append(\"close\")\n",
    "df = df[[\"close\"]]\n",
    "# df\n",
    "input_seq_len_ = 36\n",
    "output_seq_len_ = 1\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_)\n",
    "\n",
    "\n",
    "with open('../data/best_on_hyperparam_for_top_Feature_Combo_vanilla_transformer_hourly.pkl', 'rb') as file:\n",
    "    best = pickle.load(file)\n",
    "\n",
    "# instantiating model (after hyperparameter tuning)\n",
    "\n",
    "num_features = int(len(scaled_df.columns)) # a.k.a, number of cols in df\n",
    "num_layers = int(best[\"num_layers\"])\n",
    "num_heads = int(best[\"num_heads\"])\n",
    "d_model = int(best['d_model_by_num_heads']) * num_heads\n",
    "dff = int(best['dff'])\n",
    "mlp_size = int(best['mlp_size']) # size of the first MLP layer\n",
    "dropout_rate = round(best['dropout_rate'], 3)  # dropout rate for the Transformer layers\n",
    "mlp_dropout_rate = round(best['mlp_dropout_rate'], 3) # dropout rate for the MLP layers\n",
    "\n",
    "# instantiating model\n",
    "model = VanillaTimeSeriesTransformer_EncoderOnly(num_features, num_layers, d_model, num_heads, dff, mlp_size, dropout_rate, mlp_dropout_rate)\n",
    "\n",
    "# moving the model to the device (GPU if available)\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "criterion = MSELoss() # L1Loss()\n",
    "optimiser = Adam(model.parameters(), lr=round(best['lr'], 6))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "\n",
    "# declaring trainer object\n",
    "model_trainer = Trainer(model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=val_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                criterion=criterion,\n",
    "                optimiser=optimiser,\n",
    "                scheduler=scheduler,\n",
    "                device=device,\n",
    "                num_epochs=20,\n",
    "                early_stopping_patience_limit=10,\n",
    "                is_save_model=True,\n",
    "                scaler=scaler_close,\n",
    "                file_path = \"models/best_model.pt\")\n",
    "# training\n",
    "train_losses, val_losses = model_trainer.train_loop(is_plot=True, is_plot_and_plot_test=True)\n",
    "# testing\n",
    "mse, mae = model_trainer.test_model()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 2 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "with open('../data/top_feattures.pkl', 'rb') as file:\n",
    "    top_feattures = pickle.load(file)\n",
    "top_feattures = top_feattures[:2]\n",
    "top_feattures.append(\"close\")\n",
    "df = df[top_feattures]\n",
    "input_seq_len_ = 36\n",
    "output_seq_len_ = 1\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_)\n",
    "\n",
    "\n",
    "with open('../data/best_on_hyperparam_for_top_Feature_Combo_vanilla_transformer_hourly.pkl', 'rb') as file:\n",
    "    best = pickle.load(file)\n",
    "\n",
    "# instantiating model (after hyperparameter tuning)\n",
    "\n",
    "num_features = int(len(scaled_df.columns)) # a.k.a, number of cols in df\n",
    "num_layers = int(best[\"num_layers\"])\n",
    "num_heads = int(best[\"num_heads\"])\n",
    "d_model = int(best['d_model_by_num_heads']) * num_heads\n",
    "dff = int(best['dff'])\n",
    "mlp_size = int(best['mlp_size']) # size of the first MLP layer\n",
    "dropout_rate = round(best['dropout_rate'], 3)  # dropout rate for the Transformer layers\n",
    "mlp_dropout_rate = round(best['mlp_dropout_rate'], 3) # dropout rate for the MLP layers\n",
    "\n",
    "# instantiating model\n",
    "model = VanillaTimeSeriesTransformer_EncoderOnly(num_features, num_layers, d_model, num_heads, dff, mlp_size, dropout_rate, mlp_dropout_rate)\n",
    "\n",
    "# moving the model to the device (GPU if available)\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "criterion = MSELoss() # L1Loss()\n",
    "optimiser = Adam(model.parameters(), lr=round(best['lr'], 6))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "\n",
    "# declaring trainer object\n",
    "model_trainer = Trainer(model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=val_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                criterion=criterion,\n",
    "                optimiser=optimiser,\n",
    "                scheduler=scheduler,\n",
    "                device=device,\n",
    "                num_epochs=20,\n",
    "                early_stopping_patience_limit=10,\n",
    "                is_save_model=True,\n",
    "                scaler=scaler_close,\n",
    "                file_path = \"models/best_model.pt\")\n",
    "# training\n",
    "train_losses, val_losses = model_trainer.train_loop(is_plot=True, is_plot_and_plot_test=True)\n",
    "# testing\n",
    "mse, mae = model_trainer.test_model()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 5 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "with open('../data/top_feattures.pkl', 'rb') as file:\n",
    "    top_feattures = pickle.load(file)\n",
    "top_feattures = top_feattures[:5]\n",
    "top_feattures.append(\"close\")\n",
    "df = df[top_feattures]\n",
    "input_seq_len_ = 36\n",
    "output_seq_len_ = 1\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_)\n",
    "\n",
    "\n",
    "with open('../data/best_on_hyperparam_for_top_Feature_Combo_vanilla_transformer_hourly.pkl', 'rb') as file:\n",
    "    best = pickle.load(file)\n",
    "\n",
    "# instantiating model (after hyperparameter tuning)\n",
    "\n",
    "num_features = int(len(scaled_df.columns)) # a.k.a, number of cols in df\n",
    "num_layers = int(best[\"num_layers\"])\n",
    "num_heads = int(best[\"num_heads\"])\n",
    "d_model = int(best['d_model_by_num_heads']) * num_heads\n",
    "dff = int(best['dff'])\n",
    "mlp_size = int(best['mlp_size']) # size of the first MLP layer\n",
    "dropout_rate = round(best['dropout_rate'], 3)  # dropout rate for the Transformer layers\n",
    "mlp_dropout_rate = round(best['mlp_dropout_rate'], 3) # dropout rate for the MLP layers\n",
    "\n",
    "# instantiating model\n",
    "model = VanillaTimeSeriesTransformer_EncoderOnly(num_features, num_layers, d_model, num_heads, dff, mlp_size, dropout_rate, mlp_dropout_rate)\n",
    "\n",
    "# moving the model to the device (GPU if available)\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "criterion = MSELoss() # L1Loss()\n",
    "optimiser = Adam(model.parameters(), lr=round(best['lr'], 6))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "\n",
    "# declaring trainer object\n",
    "model_trainer = Trainer(model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=val_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                criterion=criterion,\n",
    "                optimiser=optimiser,\n",
    "                scheduler=scheduler,\n",
    "                device=device,\n",
    "                num_epochs=20,\n",
    "                early_stopping_patience_limit=10,\n",
    "                is_save_model=True,\n",
    "                scaler=scaler_close,\n",
    "                file_path = \"models/best_model.pt\")\n",
    "# training\n",
    "train_losses, val_losses = model_trainer.train_loop(is_plot=True, is_plot_and_plot_test=True)\n",
    "# testing\n",
    "mse, mae = model_trainer.test_model()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "with open('../data/top_feattures.pkl', 'rb') as file:\n",
    "    top_feattures = pickle.load(file)\n",
    "top_feattures.append(\"close\")\n",
    "df = df[top_feattures]\n",
    "input_seq_len_ = 36\n",
    "output_seq_len_ = 1\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_)\n",
    "\n",
    "\n",
    "with open('../data/best_on_hyperparam_for_top_Feature_Combo_vanilla_transformer_hourly.pkl', 'rb') as file:\n",
    "    best = pickle.load(file)\n",
    "\n",
    "# instantiating model (after hyperparameter tuning)\n",
    "\n",
    "num_features = int(len(scaled_df.columns)) # a.k.a, number of cols in df\n",
    "num_layers = int(best[\"num_layers\"])\n",
    "num_heads = int(best[\"num_heads\"])\n",
    "d_model = int(best['d_model_by_num_heads']) * num_heads\n",
    "dff = int(best['dff'])\n",
    "mlp_size = int(best['mlp_size']) # size of the first MLP layer\n",
    "dropout_rate = round(best['dropout_rate'], 3)  # dropout rate for the Transformer layers\n",
    "mlp_dropout_rate = round(best['mlp_dropout_rate'], 3) # dropout rate for the MLP layers\n",
    "\n",
    "# instantiating model\n",
    "model = VanillaTimeSeriesTransformer_EncoderOnly(num_features, num_layers, d_model, num_heads, dff, mlp_size, dropout_rate, mlp_dropout_rate)\n",
    "\n",
    "# moving the model to the device (GPU if available)\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "criterion = MSELoss() # L1Loss()\n",
    "optimiser = Adam(model.parameters(), lr=round(best['lr'], 6))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "\n",
    "# declaring trainer object\n",
    "model_trainer = Trainer(model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=val_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                criterion=criterion,\n",
    "                optimiser=optimiser,\n",
    "                scheduler=scheduler,\n",
    "                device=device,\n",
    "                num_epochs=20,\n",
    "                early_stopping_patience_limit=10,\n",
    "                is_save_model=True,\n",
    "                scaler=scaler_close,\n",
    "                file_path = \"models/best_model.pt\")\n",
    "# training\n",
    "train_losses, val_losses = model_trainer.train_loop(is_plot=True, is_plot_and_plot_test=True)\n",
    "# testing\n",
    "mse, mae = model_trainer.test_model()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Transformer (Next 24th Step Prediction | Encoder only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "# training\n",
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "with open('../data/top_feattures.pkl', 'rb') as file:\n",
    "    top_feattures = pickle.load(file)\n",
    "# top_feattures = top_feattures[:5]\n",
    "# top_feattures.append(\"close\")\n",
    "df = df[[\"close\"]]\n",
    "# df = df[top_feattures]\n",
    "input_seq_len_ = 168\n",
    "output_seq_len_ = 24\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_,\n",
    "                                                                                                             output_as_seq=False)\n",
    "\n",
    "\n",
    "with open('../data/best_on_hyperparam_for_top_Feature_Combo_vanilla_transformer_hourly.pkl', 'rb') as file:\n",
    "    best = pickle.load(file)\n",
    "\n",
    "# instantiating model (after hyperparameter tuning)\n",
    "\n",
    "num_features = int(len(scaled_df.columns)) # a.k.a, number of cols in df\n",
    "num_layers = int(best[\"num_layers\"])\n",
    "num_heads = int(best[\"num_heads\"])\n",
    "d_model = int(best['d_model_by_num_heads']) * num_heads\n",
    "dff = int(best['dff'])\n",
    "mlp_size = int(best['mlp_size']) # size of the first MLP layer\n",
    "dropout_rate = round(best['dropout_rate'], 3)  # dropout rate for the Transformer layers\n",
    "mlp_dropout_rate = round(best['mlp_dropout_rate'], 3) # dropout rate for the MLP layers\n",
    "\n",
    "# instantiating model\n",
    "model = VanillaTimeSeriesTransformer_EncoderOnly(num_features, num_layers, d_model, num_heads, dff, mlp_size, dropout_rate, mlp_dropout_rate)\n",
    "\n",
    "# moving the model to the device (GPU if available)\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "criterion = MSELoss() # L1Loss()\n",
    "optimiser = Adam(model.parameters(), lr=round(best['lr'], 6))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "\n",
    "# declaring trainer object\n",
    "model_trainer = Trainer(model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=val_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                criterion=criterion,\n",
    "                optimiser=optimiser,\n",
    "                scheduler=scheduler,\n",
    "                device=device,\n",
    "                num_epochs=20,\n",
    "                early_stopping_patience_limit=10,\n",
    "                is_save_model=True,\n",
    "                scaler=scaler_close,\n",
    "                file_path = \"models/best_model.pt\")\n",
    "\n",
    "# training\n",
    "train_losses, val_losses = model_trainer.train_loop(is_plot=True, is_plot_and_plot_test=True)\n",
    "# testing\n",
    "mse, mae = model_trainer.test_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete this afterwards\n",
    "# data preprocessing\n",
    "# training\n",
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "with open('../data/top_feattures.pkl', 'rb') as file:\n",
    "    top_feattures = pickle.load(file)\n",
    "# top_feattures = top_feattures[:5]\n",
    "# top_feattures.append(\"close\")\n",
    "df = df[[\"close\"]]\n",
    "# df = df[top_feattures]\n",
    "input_seq_len_ = 168\n",
    "output_seq_len_ = 1\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_,\n",
    "                                                                                                             output_as_seq=False)\n",
    "\n",
    "\n",
    "with open('../data/best_on_hyperparam_for_top_Feature_Combo_vanilla_transformer_hourly.pkl', 'rb') as file:\n",
    "    best = pickle.load(file)\n",
    "\n",
    "# instantiating model (after hyperparameter tuning)\n",
    "\n",
    "num_features = int(len(scaled_df.columns)) # a.k.a, number of cols in df\n",
    "num_layers = int(best[\"num_layers\"])\n",
    "num_heads = int(best[\"num_heads\"])\n",
    "d_model = int(best['d_model_by_num_heads']) * num_heads\n",
    "dff = int(best['dff'])\n",
    "mlp_size = int(best['mlp_size']) # size of the first MLP layer\n",
    "dropout_rate = round(best['dropout_rate'], 3)  # dropout rate for the Transformer layers\n",
    "mlp_dropout_rate = round(best['mlp_dropout_rate'], 3) # dropout rate for the MLP layers\n",
    "\n",
    "# instantiating model\n",
    "model = VanillaTimeSeriesTransformer_EncoderOnly(num_features, num_layers, d_model, num_heads, dff, mlp_size, dropout_rate, mlp_dropout_rate)\n",
    "\n",
    "# moving the model to the device (GPU if available)\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = MSELoss() # L1Loss()\n",
    "optimiser = Adam(model.parameters(), lr=round(best['lr'], 6))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "\n",
    "# declaring trainer object\n",
    "model_trainer = Trainer(model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=val_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                criterion=criterion,\n",
    "                optimiser=optimiser,\n",
    "                scheduler=scheduler,\n",
    "                device=device,\n",
    "                num_epochs=20,\n",
    "                early_stopping_patience_limit=10,\n",
    "                is_save_model=True,\n",
    "                scaler=scaler_close,\n",
    "                file_path = \"models/best_model.pt\")\n",
    "\n",
    "# training\n",
    "train_losses, val_losses = model_trainer.train_loop(is_plot=True, is_plot_and_plot_test=True)\n",
    "# testing\n",
    "mse, mae = model_trainer.test_model()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bitcoinproject1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
