{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformer import VanillaTimeSeriesTransformer\n",
    "from utils import Trainer, preprocess_data, seed_everything\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss, L1Loss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "seed_everything()\n",
    "cuda_ = 'cuda:3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Transformer (Next Step Prediction | Encoder & Decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning (Structural Tuning Too) for 2 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "df = df[[\"close\"]]\n",
    "input_seq_len_ = 36\n",
    "output_seq_len_ = 1\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_,\n",
    "                                                                                                             output_as_seq=False)\n",
    "\n",
    "def objective(space):\n",
    "    start_time = int(time.time())\n",
    "    print(f\"\"\"\n",
    "    device: {torch.device(cuda_ if torch.cuda.is_available() else 'cpu')},\n",
    "    num_heads:  {space['num_heads']},\n",
    "    d_model: {space['d_model_by_num_heads'] * space['num_heads']},\n",
    "    num_layers: {space['num_layers']},\n",
    "    dff: {space['dff']},\n",
    "    mlp_size: {space['mlp_size']},\n",
    "    dropout_rate: {space['dropout_rate']},\n",
    "    mlp_dropout_rate: {space['mlp_dropout_rate']}\"\"\")\n",
    "\n",
    "    device = torch.device(cuda_ if torch.cuda.is_available() else 'cpu')\n",
    "    num_heads = space['num_heads']\n",
    "    d_model = space['d_model_by_num_heads'] * num_heads\n",
    "\n",
    "    params = {\n",
    "        'num_features': int(len(scaled_df.columns)),\n",
    "        'd_model': d_model, \n",
    "        'num_layers': space['num_layers'], \n",
    "        'dff': space['dff'], \n",
    "        'dropout_rate': space['dropout_rate'], \n",
    "        'mlp_size': space['mlp_size'], \n",
    "        'mlp_dropout_rate': space['mlp_dropout_rate'],\n",
    "        \"num_heads\":space['num_heads']}\n",
    "    model = VanillaTimeSeriesTransformer(**params)\n",
    "    device = torch.device(cuda_ if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimiser = Adam(model.parameters(), lr=space['lr'])\n",
    "    scheduler = ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "    criterion = MSELoss()\n",
    "\n",
    "    model_trainer = Trainer(model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=val_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                criterion=criterion,\n",
    "                optimiser=optimiser,\n",
    "                scheduler=scheduler,\n",
    "                device=device,\n",
    "                num_epochs=50,\n",
    "                early_stopping_patience_limit=10,\n",
    "                is_save_model=True,\n",
    "                scaler=scaler_close,\n",
    "                file_path = f\"models/best_model_{start_time}.pt\")\n",
    "    train_losses, val_losses = model_trainer.train_loop()\n",
    "\n",
    "    return {'loss': val_losses[-1], 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "space = {\n",
    "    'num_layers': scope.int(hp.quniform('num_layers', 1, 8, 1)),\n",
    "    'num_heads': scope.int(hp.quniform('num_heads', 1, 8, 1)),\n",
    "    'd_model_by_num_heads': scope.int(hp.quniform('d_model_by_num_heads', 32, 64, 2)),\n",
    "    'dff': scope.int(hp.quniform('dff', 2, 2048, 50)),\n",
    "    'mlp_size': scope.int(hp.quniform('mlp_size', 32, 64, 2)),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.1, 0.3),\n",
    "    'mlp_dropout_rate': hp.uniform('mlp_dropout_rate', 0.1, 0.3),\n",
    "    'lr': hp.loguniform('lr', np.log(0.0001), np.log(0.1))\n",
    "}\n",
    "\n",
    "# Run the optimization\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(best)\n",
    "\n",
    "with open('../data/stats_on_hyperparam_for_two_cols_vanilla_transformer_hourly_encoder_decoder.pkl', 'wb') as file:\n",
    "    pickle.dump(best, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "with open('../data/stats_on_hyperparam_for_two_cols_vanilla_transformer_hourly_encoder_decoder.pkl', 'rb') as file:\n",
    "    best = pickle.load(file)\n",
    "\n",
    "\n",
    "input_seq_len_ = 36\n",
    "output_seq_len_ = 1\n",
    "stats = {}\n",
    "\n",
    "for cols in df.columns:\n",
    "  if cols != \"close\":\n",
    "    print(cols)\n",
    "\n",
    "    temp_df = df[[\"close\", cols]]\n",
    "    scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(temp_df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_,\n",
    "                                                                                                             output_as_seq=False)\n",
    "    \n",
    "\n",
    "    params = {\n",
    "        'num_features': int(len(scaled_df.columns)),\n",
    "        'd_model': int(best['d_model_by_num_heads']) * int(best[\"num_heads\"]), \n",
    "        'num_layers': int(best[\"num_layers\"]), \n",
    "        'dff': int(best['dff']), \n",
    "        'dropout_rate': round(best['dropout_rate'], 3), \n",
    "        'mlp_size': int(best['mlp_size']), \n",
    "        'mlp_dropout_rate': round(best['mlp_dropout_rate'], 3),\n",
    "        \"num_heads\":int(best[\"num_heads\"])}\n",
    "    \n",
    "    model = VanillaTimeSeriesTransformer(**params)\n",
    "    device = torch.device(cuda_ if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimiser = Adam(model.parameters(), lr=best['lr'])\n",
    "    scheduler = ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "    criterion = MSELoss()\n",
    "\n",
    "    model_trainer = Trainer(model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=val_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                criterion=criterion,\n",
    "                optimiser=optimiser,\n",
    "                scheduler=scheduler,\n",
    "                device=device,\n",
    "                num_epochs=50,\n",
    "                early_stopping_patience_limit=10,\n",
    "                is_save_model=True,\n",
    "                scaler=scaler_close,\n",
    "                file_path = \"models/best_model_.pt\")\n",
    "    \n",
    "    train_losses, val_losses = model_trainer.train_loop()\n",
    "    # testing\n",
    "    mse, mae = model_trainer.test_model()\n",
    "\n",
    "    stats[cols] = {\n",
    "        \"mse\":mse,\n",
    "        \"mae\":mae\n",
    "    }\n",
    "\n",
    "with open('../data/stats_on_features_vanilla_transformer_hourly_encoder_decoder.pkl', 'wb') as file:\n",
    "    pickle.dump(stats, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/stats_on_features_vanilla_transformer_hourly_encoder_decoder.pkl', 'rb') as file:\n",
    "    loaded_stats = pickle.load(file)\n",
    "sorted_loaded_stats = OrderedDict(sorted(loaded_stats.items(), key=lambda item: item[1]['mse']))\n",
    "\n",
    "count = 0\n",
    "top_feattures = []\n",
    "for k,v in sorted_loaded_stats.items():\n",
    "  if count < 10:\n",
    "    top_feattures.append(k)\n",
    "    count += 1\n",
    "print(top_feattures)\n",
    "with open('../data/top_feattures_encoder_decoder.pkl', 'wb') as file:\n",
    "    pickle.dump(top_feattures, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning (Structural Tuning Too) for Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "with open('../data/top_feattures_encoder_decoder.pkl', 'rb') as file:\n",
    "    top_feattures = pickle.load(file)\n",
    "top_feattures.append(\"close\")\n",
    "df = df[top_feattures]\n",
    "input_seq_len_ = 36\n",
    "output_seq_len_ = 1\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_,\n",
    "                                                                                                             output_as_seq=False)\n",
    "\n",
    "def objective(space):\n",
    "    start_time = int(time.time())\n",
    "    print(f\"\"\"\n",
    "    device: {torch.device(cuda_ if torch.cuda.is_available() else 'cpu')},\n",
    "    num_heads:  {space['num_heads']},\n",
    "    d_model: {space['d_model_by_num_heads'] * space['num_heads']},\n",
    "    num_layers: {space['num_layers']},\n",
    "    dff: {space['dff']},\n",
    "    mlp_size: {space['mlp_size']},\n",
    "    dropout_rate: {space['dropout_rate']},\n",
    "    mlp_dropout_rate: {space['mlp_dropout_rate']}\"\"\")\n",
    "\n",
    "    device = torch.device(cuda_ if torch.cuda.is_available() else 'cpu')\n",
    "    num_heads = space['num_heads']\n",
    "    d_model = space['d_model_by_num_heads'] * num_heads\n",
    "\n",
    "    params = {\n",
    "        'num_features': int(len(scaled_df.columns)),\n",
    "        'd_model': d_model, \n",
    "        'num_layers': space['num_layers'], \n",
    "        'dff': space['dff'], \n",
    "        'dropout_rate': space['dropout_rate'], \n",
    "        'mlp_size': space['mlp_size'], \n",
    "        'mlp_dropout_rate': space['mlp_dropout_rate'],\n",
    "        \"num_heads\":space['num_heads']}\n",
    "    model = VanillaTimeSeriesTransformer(**params)\n",
    "    device = torch.device(cuda_ if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimiser = Adam(model.parameters(), lr=space['lr'])\n",
    "    scheduler = ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "    criterion = MSELoss()\n",
    "\n",
    "    model_trainer = Trainer(model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=val_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                criterion=criterion,\n",
    "                optimiser=optimiser,\n",
    "                scheduler=scheduler,\n",
    "                device=device,\n",
    "                num_epochs=50,\n",
    "                early_stopping_patience_limit=10,\n",
    "                is_save_model=True,\n",
    "                scaler=scaler_close,\n",
    "                file_path = f\"models/best_model_{start_time}.pt\")\n",
    "    train_losses, val_losses = model_trainer.train_loop()\n",
    "\n",
    "    return {'loss': val_losses[-1], 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "space = {\n",
    "    'num_layers': scope.int(hp.quniform('num_layers', 1, 8, 1)),\n",
    "    'num_heads': scope.int(hp.quniform('num_heads', 1, 8, 1)),\n",
    "    'd_model_by_num_heads': scope.int(hp.quniform('d_model_by_num_heads', 32, 64, 2)),\n",
    "    'dff': scope.int(hp.quniform('dff', 2, 2048, 50)),\n",
    "    'mlp_size': scope.int(hp.quniform('mlp_size', 32, 64, 2)),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.1, 0.3),\n",
    "    'mlp_dropout_rate': hp.uniform('mlp_dropout_rate', 0.1, 0.3),\n",
    "    'lr': hp.loguniform('lr', np.log(0.0001), np.log(0.1))\n",
    "}\n",
    "\n",
    "# Run the optimization\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(best)\n",
    "\n",
    "with open('../data/best_on_hyperparam_for_top_Feature_Combo_vanilla_transformer_encoder_decoder_hourly.pkl', 'wb') as file:\n",
    "    pickle.dump(best, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation of Vanilla Transformer (Encoder-Decoder) on Close Price, Top 2, 5, and 10 Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close Price Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "df = df[\"close\"]\n",
    "input_seq_len_ = 36\n",
    "output_seq_len_ = 1\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_,\n",
    "                                                                                                             output_as_seq=False)\n",
    "with open('../data/best_on_hyperparam_for_top_Feature_Combo_vanilla_transformer_encoder_decoder_hourly.pkl', 'rb') as file:\n",
    "    best = pickle.load(file)\n",
    "\n",
    "\n",
    "params = {\n",
    "    'num_features': int(len(scaled_df.columns)),\n",
    "    'd_model': int(best['d_model_by_num_heads']) * int(best[\"num_heads\"]), \n",
    "    'num_layers': int(best[\"num_layers\"]), \n",
    "    'dff': int(best['dff']), \n",
    "    'dropout_rate': round(best['dropout_rate'], 3), \n",
    "    'mlp_size': int(best['mlp_size']), \n",
    "    'mlp_dropout_rate': round(best['mlp_dropout_rate'], 3),\n",
    "    \"num_heads\":int(best[\"num_heads\"])}\n",
    "\n",
    "model = VanillaTimeSeriesTransformer(**params)\n",
    "device = torch.device(cuda_ if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "optimiser = Adam(model.parameters(), lr=best['lr'])\n",
    "scheduler = ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "criterion = MSELoss()\n",
    "\n",
    "model_trainer = Trainer(model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            criterion=criterion,\n",
    "            optimiser=optimiser,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            num_epochs=50,\n",
    "            early_stopping_patience_limit=10,\n",
    "            is_save_model=True,\n",
    "            scaler=scaler_close,\n",
    "            file_path = \"models/best_model_.pt\")\n",
    "\n",
    "train_losses, val_losses = model_trainer.train_loop()\n",
    "# testing\n",
    "mse, mae = model_trainer.test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 2 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "with open('../data/top_feattures_encoder_decoder.pkl', 'rb') as file:\n",
    "    top_feattures = pickle.load(file)\n",
    "top_feattures = top_feattures[:2]\n",
    "top_feattures.append(\"close\")\n",
    "df = df[top_feattures]\n",
    "input_seq_len_ = 36\n",
    "output_seq_len_ = 1\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_,\n",
    "                                                                                                             output_as_seq=False)\n",
    "\n",
    "\n",
    "with open('../data/best_on_hyperparam_for_top_Feature_Combo_vanilla_transformer_encoder_decoder_hourly.pkl', 'rb') as file:\n",
    "    best = pickle.load(file)\n",
    "\n",
    "params = {\n",
    "    'num_features': int(len(scaled_df.columns)),\n",
    "    'd_model': int(best['d_model_by_num_heads']) * int(best[\"num_heads\"]), \n",
    "    'num_layers': int(best[\"num_layers\"]), \n",
    "    'dff': int(best['dff']), \n",
    "    'dropout_rate': round(best['dropout_rate'], 3), \n",
    "    'mlp_size': int(best['mlp_size']), \n",
    "    'mlp_dropout_rate': round(best['mlp_dropout_rate'], 3),\n",
    "    \"num_heads\":int(best[\"num_heads\"])}\n",
    "\n",
    "model = VanillaTimeSeriesTransformer(**params)\n",
    "device = torch.device(cuda_ if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "optimiser = Adam(model.parameters(), lr=best['lr'])\n",
    "scheduler = ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "criterion = MSELoss()\n",
    "\n",
    "model_trainer = Trainer(model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            criterion=criterion,\n",
    "            optimiser=optimiser,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            num_epochs=50,\n",
    "            early_stopping_patience_limit=10,\n",
    "            is_save_model=True,\n",
    "            scaler=scaler_close,\n",
    "            file_path = \"models/best_model_.pt\")\n",
    "\n",
    "train_losses, val_losses = model_trainer.train_loop()\n",
    "# testing\n",
    "mse, mae = model_trainer.test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 5 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "with open('../data/top_feattures_encoder_decoder.pkl', 'rb') as file:\n",
    "    top_feattures = pickle.load(file)\n",
    "top_feattures = top_feattures[:5]\n",
    "top_feattures.append(\"close\")\n",
    "df = df[top_feattures]\n",
    "input_seq_len_ = 36\n",
    "output_seq_len_ = 1\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_,\n",
    "                                                                                                             output_as_seq=False)\n",
    "\n",
    "\n",
    "with open('../data/best_on_hyperparam_for_top_Feature_Combo_vanilla_transformer_encoder_decoder_hourly.pkl', 'rb') as file:\n",
    "    best = pickle.load(file)\n",
    "\n",
    "params = {\n",
    "    'num_features': int(len(scaled_df.columns)),\n",
    "    'd_model': int(best['d_model_by_num_heads']) * int(best[\"num_heads\"]), \n",
    "    'num_layers': int(best[\"num_layers\"]), \n",
    "    'dff': int(best['dff']), \n",
    "    'dropout_rate': round(best['dropout_rate'], 3), \n",
    "    'mlp_size': int(best['mlp_size']), \n",
    "    'mlp_dropout_rate': round(best['mlp_dropout_rate'], 3),\n",
    "    \"num_heads\":int(best[\"num_heads\"])}\n",
    "\n",
    "model = VanillaTimeSeriesTransformer(**params)\n",
    "device = torch.device(cuda_ if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "optimiser = Adam(model.parameters(), lr=best['lr'])\n",
    "scheduler = ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "criterion = MSELoss()\n",
    "\n",
    "model_trainer = Trainer(model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            criterion=criterion,\n",
    "            optimiser=optimiser,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            num_epochs=50,\n",
    "            early_stopping_patience_limit=10,\n",
    "            is_save_model=True,\n",
    "            scaler=scaler_close,\n",
    "            file_path = \"models/best_model_.pt\")\n",
    "\n",
    "train_losses, val_losses = model_trainer.train_loop()\n",
    "# testing\n",
    "mse, mae = model_trainer.test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Final_data_hourly.csv\")\n",
    "with open('../data/top_feattures_encoder_decoder.pkl', 'rb') as file:\n",
    "    top_feattures = pickle.load(file)\n",
    "# top_feattures = top_feattures[:5]\n",
    "top_feattures.append(\"close\")\n",
    "df = df[top_feattures]\n",
    "input_seq_len_ = 36\n",
    "output_seq_len_ = 1\n",
    "scaled_df, scaler_general, scaler_close, train_dataloader, val_dataloader, test_dataloader = preprocess_data(df,\n",
    "                                                                                                             batch_size = 256,\n",
    "                                                                                                             input_seq_len=input_seq_len_,\n",
    "                                                                                                             output_seq_len=output_seq_len_,\n",
    "                                                                                                             output_as_seq=False)\n",
    "\n",
    "\n",
    "with open('../data/best_on_hyperparam_for_top_Feature_Combo_vanilla_transformer_encoder_decoder_hourly.pkl', 'rb') as file:\n",
    "    best = pickle.load(file)\n",
    "\n",
    "params = {\n",
    "    'num_features': int(len(scaled_df.columns)),\n",
    "    'd_model': int(best['d_model_by_num_heads']) * int(best[\"num_heads\"]), \n",
    "    'num_layers': int(best[\"num_layers\"]), \n",
    "    'dff': int(best['dff']), \n",
    "    'dropout_rate': round(best['dropout_rate'], 3), \n",
    "    'mlp_size': int(best['mlp_size']), \n",
    "    'mlp_dropout_rate': round(best['mlp_dropout_rate'], 3),\n",
    "    \"num_heads\":int(best[\"num_heads\"])}\n",
    "\n",
    "model = VanillaTimeSeriesTransformer(**params)\n",
    "device = torch.device(cuda_ if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "optimiser = Adam(model.parameters(), lr=best['lr'])\n",
    "scheduler = ReduceLROnPlateau(optimiser, 'min', factor=0.9, patience=5)\n",
    "criterion = MSELoss()\n",
    "\n",
    "model_trainer = Trainer(model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            criterion=criterion,\n",
    "            optimiser=optimiser,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            num_epochs=50,\n",
    "            early_stopping_patience_limit=10,\n",
    "            is_save_model=True,\n",
    "            scaler=scaler_close,\n",
    "            file_path = \"models/best_model_.pt\")\n",
    "\n",
    "train_losses, val_losses = model_trainer.train_loop()\n",
    "# testing\n",
    "mse, mae = model_trainer.test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Transformer (Next 24th Step Prediction | Encoder - Decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bitcoinproject1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
